{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24359182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, average_precision_score\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "674977f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading dataset\n",
    "\n",
    "df = pd.read_csv(\"dataset/fb-forum.txt\", sep=\",\",names=['src','dst','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd973de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the Dataset in Snapshots\n",
    "\n",
    "df1=df.iloc[0:6745]\n",
    "df2=df.iloc[6745:13489]\n",
    "df3=df.iloc[13489:20233]\n",
    "df4=df.iloc[20233:26977]\n",
    "df5=df.iloc[26977:]\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df4 = df4.reset_index(drop=True)\n",
    "df5 = df5.reset_index(drop=True)\n",
    "\n",
    "#print(df1,df2,df3,df4,df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8ec8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#G\n",
    "G= nx.Graph()\n",
    "\n",
    "src= df['src']\n",
    "dst= df['dst']\n",
    "\n",
    "for i in range (src.size):\n",
    "        s=src[i]\n",
    "        d=dst[i]\n",
    "        G.add_edges_from([(s,d)])\n",
    "\n",
    "G_nodes= list(G.nodes())\n",
    "\n",
    "for i in range(src.size):\n",
    "    if src[i] in G_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G.add_node(src[i])\n",
    "\n",
    "for i in range(dst.size):\n",
    "    if dst[i] in G_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G.add_node(dst[i])\n",
    "        \n",
    "G_nodes= list(G.nodes())\n",
    "\n",
    "\n",
    "#nx.draw_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d50fa998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#G1\n",
    "G1= nx.Graph()\n",
    "\n",
    "src1= df1['src']\n",
    "dst1= df1['dst']\n",
    "\n",
    "for i in range (src1.size):\n",
    "        s=src1[i]\n",
    "        d=dst1[i]\n",
    "        G1.add_edges_from([(s,d)])\n",
    "\n",
    "G1_nodes= list(G1.nodes())\n",
    "\n",
    "for i in range(src1.size):\n",
    "    if src1[i] in G1_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G1.add_node(src1[i])\n",
    "\n",
    "for i in range(dst1.size):\n",
    "    if dst1[i] in G1_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G1.add_node(dst1[i])\n",
    "        \n",
    "G1_nodes= list(G1.nodes())\n",
    "\n",
    "\n",
    "#nx.draw_networkx(G1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bc968c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#G2\n",
    "G2= nx.Graph()\n",
    "\n",
    "src2= df2['src']\n",
    "dst2= df2['dst']\n",
    "\n",
    "for i in range (src2.size):\n",
    "        s=src2[i]\n",
    "        d=dst2[i]\n",
    "        G2.add_edges_from([(s,d)])\n",
    "\n",
    "G2_nodes= list(G2.nodes())\n",
    "\n",
    "for i in range(src2.size):\n",
    "    if src2[i] in G2_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G2.add_node(src2[i])\n",
    "\n",
    "for i in range(dst2.size):\n",
    "    if dst2[i] in G2_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G2.add_node(dst2[i])\n",
    "        \n",
    "G2_nodes= list(G2.nodes())\n",
    "\n",
    "#nx.draw_networkx(G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0111332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#G3\n",
    "G3= nx.Graph()\n",
    "\n",
    "src3= df3['src']\n",
    "dst3= df3['dst']\n",
    "\n",
    "for i in range (src3.size):\n",
    "        s=src3[i]\n",
    "        d=dst3[i]\n",
    "        G3.add_edges_from([(s,d)])\n",
    "\n",
    "G3_nodes= list(G3.nodes())\n",
    "\n",
    "for i in range(src3.size):\n",
    "    if src3[i] in G3_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G3.add_node(src3[i])\n",
    "\n",
    "for i in range(dst3.size):\n",
    "    if dst3[i] in G3_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G3.add_node(dst3[i])\n",
    "        \n",
    "G3_nodes= list(G3.nodes())\n",
    "\n",
    "#nx.draw_networkx(G3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3205649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#G4\n",
    "G4= nx.Graph()\n",
    "\n",
    "src4= df4['src']\n",
    "dst4= df4['dst']\n",
    "\n",
    "for i in range (src4.size):\n",
    "        s=src4[i]\n",
    "        d=dst4[i]\n",
    "        G4.add_edges_from([(s,d)])\n",
    "\n",
    "G4_nodes= list(G4.nodes())\n",
    "\n",
    "for i in range(src4.size):\n",
    "    if src4[i] in G4_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G4.add_node(src4[i])\n",
    "\n",
    "for i in range(dst4.size):\n",
    "    if dst4[i] in G4_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G4.add_node(dst4[i])\n",
    "        \n",
    "G4_nodes= list(G4.nodes())\n",
    "\n",
    "#nx.draw_networkx(G4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fecca5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#G5\n",
    "G5= nx.Graph()\n",
    "\n",
    "src5= df5['src']\n",
    "dst5= df5['dst']\n",
    "\n",
    "for i in range (src5.size):\n",
    "        s=src5[i]\n",
    "        d=dst5[i]\n",
    "        G5.add_edges_from([(s,d)])\n",
    "\n",
    "G5_nodes= list(G5.nodes())\n",
    "\n",
    "for i in range(src5.size):\n",
    "    if src5[i] in G5_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G5.add_node(src5[i])\n",
    "\n",
    "for i in range(dst5.size):\n",
    "    if dst5[i] in G5_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G5.add_node(dst5[i])\n",
    "        \n",
    "G5_nodes= list(G5.nodes())\n",
    "\n",
    "\n",
    "#nx.draw_networkx(G5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2f62fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#G5\n",
    "for i in range(len(G_nodes)):\n",
    "    if G_nodes[i] in G5_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G5.add_node(G_nodes[i])\n",
    "        \n",
    "G5_nodes= list(G5.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22f4241e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 899\n",
      "Total edges: 1758\n",
      "Training edges (positive): 1230\n",
      "Training edges (negative): 1230\n",
      "Test edges (positive): 527\n",
      "Test edges (negative): 527\n"
     ]
    }
   ],
   "source": [
    "# Preparing Train and Test Datasets\n",
    "from gae.preprocessing import mask_test_edges\n",
    "np.random.seed(0) # make sure train-test split is consistent between notebooks\n",
    "\n",
    "adj_sparse = nx.to_scipy_sparse_matrix(G5)\n",
    "\n",
    "adj_train, train_edges, train_edges_false, val_edges, val_edges_false, \\\n",
    "    test_edges, test_edges_false = mask_test_edges(adj_sparse, test_frac=.3, val_frac=.0, prevent_disconnect = True)\n",
    "\n",
    "print (\"Total nodes:\", adj_sparse.shape[0])\n",
    "print (\"Total edges:\", int(adj_sparse.nnz/2)) # adj is symmetric, so nnz (num non-zero) = 2*num_edges\n",
    "print (\"Training edges (positive):\", len(train_edges))\n",
    "print (\"Training edges (negative):\", len(train_edges_false))\n",
    "print (\"Test edges (positive):\", len(test_edges))\n",
    "print (\"Test edges (negative):\", len(test_edges_false))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8157a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adding all nodes to snapshot\n",
    "\n",
    "#G1\n",
    "for i in range(len(G_nodes)):\n",
    "    if G_nodes[i] in G1_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G1.add_node(G_nodes[i])\n",
    "        \n",
    "G1_nodes= list(G1.nodes())\n",
    "\n",
    "#G2\n",
    "for i in range(len(G_nodes)):\n",
    "    if G_nodes[i] in G2_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G2.add_node(G_nodes[i])\n",
    "        \n",
    "G2_nodes= list(G2.nodes())\n",
    "\n",
    "#G3\n",
    "for i in range(len(G_nodes)):\n",
    "    if G_nodes[i] in G3_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G3.add_node(G_nodes[i])\n",
    "        \n",
    "G3_nodes= list(G3.nodes())\n",
    "\n",
    "#G4\n",
    "for i in range(len(G_nodes)):\n",
    "    if G_nodes[i] in G4_nodes:\n",
    "        continue\n",
    "    else:\n",
    "        G4.add_node(G_nodes[i])\n",
    "        \n",
    "G4_nodes= list(G4.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d429e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Removing test edges from Snapshots\n",
    "\n",
    "#G1\n",
    "\n",
    "G1_edges=G1.edges()\n",
    "\n",
    "for i in range(len(test_edges)):\n",
    "    if test_edges[i] in G1_edges:\n",
    "        G1.remove_edge(*test_edges[i])\n",
    "\n",
    "\n",
    " #G2\n",
    "    \n",
    "G2_edges=G2.edges()\n",
    "\n",
    "\n",
    "for i in range(len(test_edges)):\n",
    "    if test_edges[i] in G2_edges:\n",
    "        G2.remove_edge(*test_edges[i])\n",
    "\n",
    "\n",
    "#G3\n",
    "\n",
    "G3_edges=G3.edges()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(test_edges)):\n",
    "    if test_edges[i] in G3_edges:\n",
    "        G3.remove_edge(*test_edges[i])\n",
    "\n",
    "\n",
    "\n",
    "#G4\n",
    "\n",
    "G4_edges=G4.edges()\n",
    "\n",
    "for i in range(len(test_edges)):\n",
    "    if test_edges[i] in G4_edges:\n",
    "        G4.remove_edge(*test_edges[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e25fa78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e67209392084de69a9eadfe26d71297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/899 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|███████████████████████████████████████████████████████| 12/12 [00:05<00:00,  2.37it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713fa5446bfe442dab01c762e1da1183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/899 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|███████████████████████████████████████████████████████| 12/12 [00:04<00:00,  2.68it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1edc0242bd2497fb3422807dff69da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/899 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|███████████████████████████████████████████████████████| 12/12 [00:05<00:00,  2.05it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90af031b90da48b6adca4b8c3c75ba94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/899 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|███████████████████████████████████████████████████████| 12/12 [00:05<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "#Getting node embeddings\n",
    "\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "#G1\n",
    "node2vec_1 = Node2Vec(G1, dimensions=128, walk_length=80, num_walks=12)\n",
    "n2v_model_1 = node2vec_1.fit(window=10, min_count=1)\n",
    "\n",
    "#G2\n",
    "node2vec_2 = Node2Vec(G2, dimensions=128, walk_length=80, num_walks=12)\n",
    "n2v_model_2 = node2vec_2.fit(window=10, min_count=1)\n",
    "\n",
    "#G3\n",
    "node2vec_3 = Node2Vec(G3, dimensions=128, walk_length=80, num_walks=12)\n",
    "n2v_model_3 = node2vec_3.fit(window=10, min_count=1)\n",
    "\n",
    "#G4\n",
    "node2vec_4 = Node2Vec(G4, dimensions=128, walk_length=80, num_walks=12)\n",
    "n2v_model_4 = node2vec_4.fit(window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6f70927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting edge embeddings\n",
    "\n",
    "def get_edge_embeddings(edge_list,n2v_model):\n",
    "    embs = []\n",
    "    for edge in edge_list:\n",
    "        node1 = edge[0]\n",
    "        node2 = edge[1]\n",
    "        emb1 = n2v_model.wv[node1]\n",
    "        emb2 = n2v_model.wv[node2]\n",
    "        edge_emb = np.add(emb1, emb2)\n",
    "        embs.append(edge_emb)\n",
    "    embs = np.array(embs)\n",
    "    return embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "251e1ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train dataset\n",
    "\n",
    "#Positive\n",
    "\n",
    "pos_train_embs_1= get_edge_embeddings(train_edges,n2v_model_1)\n",
    "pos_train_embs_2= get_edge_embeddings(train_edges,n2v_model_2)\n",
    "pos_train_embs_3= get_edge_embeddings(train_edges,n2v_model_3)\n",
    "pos_train_embs_4= get_edge_embeddings(train_edges,n2v_model_4)\n",
    "\n",
    "#Negative\n",
    "\n",
    "neg_train_embs_1= get_edge_embeddings(train_edges_false,n2v_model_1)\n",
    "neg_train_embs_2= get_edge_embeddings(train_edges_false,n2v_model_2)\n",
    "neg_train_embs_3= get_edge_embeddings(train_edges_false,n2v_model_3)\n",
    "neg_train_embs_4= get_edge_embeddings(train_edges_false,n2v_model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "806aa6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test dataset\n",
    "\n",
    "#Positive\n",
    "\n",
    "pos_test_embs_1= get_edge_embeddings(test_edges,n2v_model_1)\n",
    "pos_test_embs_2= get_edge_embeddings(test_edges,n2v_model_2)\n",
    "pos_test_embs_3= get_edge_embeddings(test_edges,n2v_model_3)\n",
    "pos_test_embs_4= get_edge_embeddings(test_edges,n2v_model_4)\n",
    "\n",
    "#Negative\n",
    "\n",
    "neg_test_embs_1= get_edge_embeddings(test_edges_false,n2v_model_1)\n",
    "neg_test_embs_2= get_edge_embeddings(test_edges_false,n2v_model_2)\n",
    "neg_test_embs_3= get_edge_embeddings(test_edges_false,n2v_model_3)\n",
    "neg_test_embs_4= get_edge_embeddings(test_edges_false,n2v_model_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83a4170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Dataset\n",
    "train_edges_emb_1=np.hstack([pos_train_embs_1,pos_train_embs_2,pos_train_embs_3,pos_train_embs_4])\n",
    "train_edges_label_1=np.ones(len(train_edges))\n",
    "train_edges_emb_2=np.hstack([neg_train_embs_1,neg_train_embs_2,neg_train_embs_3,neg_train_embs_4])\n",
    "train_edges_label_2=np.zeros(len(train_edges_false))\n",
    "\n",
    "train_edges_label= np.concatenate([train_edges_label_1,train_edges_label_2])\n",
    "train_edges_emb= np.concatenate([train_edges_emb_1,train_edges_emb_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44b0c31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Dataset\n",
    "test_edges_emb_1=np.hstack([pos_test_embs_1,pos_test_embs_2,pos_test_embs_3,pos_test_embs_4])\n",
    "test_edges_label_1=np.ones(len(test_edges))\n",
    "test_edges_emb_2=np.hstack([neg_test_embs_1,neg_test_embs_2,neg_test_embs_3,neg_test_embs_4])\n",
    "test_edges_label_2=np.zeros(len(test_edges_false))\n",
    "\n",
    "test_edges_label= np.concatenate([test_edges_label_1,test_edges_label_2])\n",
    "test_edges_emb= np.concatenate([test_edges_emb_1,test_edges_emb_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef2d99ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8753713152029496\n",
      "0.8365510606559432\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression classifier on train-set edge embeddings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "edge_classifier = LogisticRegression(max_iter=3000,random_state=0)\n",
    "edge_classifier.fit(train_edges_emb, train_edges_label)\n",
    "\n",
    "# Predicted edge scores: probability of being of class \"1\" (real edge)\n",
    "test_preds = edge_classifier.predict_proba(test_edges_emb)[:, 1]\n",
    "test_roc = roc_auc_score(test_edges_label, test_preds)\n",
    "test_ap = average_precision_score(test_edges_label, test_preds)\n",
    "print(test_roc)\n",
    "print(test_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4ce43f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8598777945407214\n",
      "0.8301993917025384\n"
     ]
    }
   ],
   "source": [
    "edge_classifier_1= RandomForestClassifier(n_estimators = 50)\n",
    "edge_classifier_1.fit(train_edges_emb, train_edges_label)\n",
    "\n",
    "test_preds_1 = edge_classifier_1.predict_proba(test_edges_emb)[:, 1]\n",
    "test_roc_1 = roc_auc_score(test_edges_label, test_preds_1)\n",
    "test_ap_1 = average_precision_score(test_edges_label, test_preds_1)\n",
    "\n",
    "print(test_roc_1)\n",
    "print(test_ap_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77dc2d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8511966701352758\n",
      "0.7962973090906847\n"
     ]
    }
   ],
   "source": [
    "edge_classifier_2 = GradientBoostingClassifier(n_estimators=50, learning_rate=0.5, max_depth=8, random_state=0).fit(train_edges_emb, train_edges_label)\n",
    "edge_classifier_2 = GradientBoostingClassifier(n_estimators=50, learning_rate=0.5, max_depth=8, random_state=0).fit(train_edges_emb, train_edges_label)\n",
    "\n",
    "test_preds_2 = edge_classifier_2.predict_proba(test_edges_emb)[:, 1]\n",
    "test_roc_2 = roc_auc_score(test_edges_label, test_preds_2)\n",
    "test_ap_2 = average_precision_score(test_edges_label, test_preds_2)\n",
    "\n",
    "print(test_roc_2)\n",
    "print(test_ap_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db55bc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7903225806451613\n",
      "0.7903225806451613\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc8dcfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train Random Forest classifier on train-set edge embeddings\n",
    "\n",
    "# #classifier\n",
    "# clf1 = RandomForestClassifier()\n",
    " \n",
    "# # parameters\n",
    "# param = {'n_estimators' : [10,50,100], 'max_depth' : [5,10,15]}\n",
    " \n",
    "# # model\n",
    "# grid_clf_acc1 = GridSearchCV(clf1, param_grid = param)\n",
    " \n",
    "# # train the model\n",
    "# grid_clf_acc1.fit(train_edges_emb,train_edges_label)\n",
    " \n",
    "# print('Grid best parameter (max. accuracy): ', grid_clf_acc1.best_params_)\n",
    "# print('Grid best score (accuracy): ', grid_clf_acc1.best_score_)\n",
    " \n",
    "# # alternative metric to optimize over grid parameters: AUC\n",
    "# grid_clf_auc1 = GridSearchCV(clf1, param_grid = param, scoring = 'roc_auc')\n",
    "# grid_clf_auc1.fit(train_edges_emb,train_edges_label)\n",
    "# predict_proba = grid_clf_auc1.predict_proba(test_edges_emb)[:,1]\n",
    " \n",
    "# print('Test set AUC: ', roc_auc_score(test_edges_label, predict_proba))\n",
    "# print('Grid best parameter (max. AUC): ', grid_clf_auc1.best_params_)\n",
    "# print('Grid best score (AUC): ', grid_clf_auc1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0604a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train Gradient Boost classifier on train-set edge embeddings\n",
    "\n",
    "# # classifier\n",
    "# clf2 = GradientBoostingClassifier()\n",
    " \n",
    "# # parameters\n",
    "# param = {'learning_rate' : [.05,.1]}\n",
    " \n",
    "# # model\n",
    "# grid_clf_acc2 = GridSearchCV(clf2, param_grid = param)\n",
    " \n",
    "# # train the model\n",
    "# grid_clf_acc2.fit(train_edges_emb,train_edges_label)\n",
    " \n",
    "# print('Grid best parameter (max. accuracy): ', grid_clf_acc2.best_params_)\n",
    "# print('Grid best score (accuracy): ', grid_clf_acc2.best_score_)\n",
    " \n",
    "# # alternative metric to optimize over grid parameters: AUC\n",
    "# grid_clf_auc2 = GridSearchCV(clf2, param_grid = param, scoring = 'roc_auc')\n",
    "# grid_clf_auc2.fit(train_edges_emb,train_edges_label)\n",
    "# predict_proba = grid_clf_auc2.predict_proba(test_edges_emb)[:,1]\n",
    " \n",
    "# print('Test set AUC: ', roc_auc_score(test_edges_label, predict_proba))\n",
    "# print('Grid best parameter (max. AUC): ', grid_clf_auc2.best_params_)\n",
    "# print('Grid best score (AUC): ', grid_clf_auc2.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da1de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
